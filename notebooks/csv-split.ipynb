{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from thefuzz import process\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"opendialkg.csv\"\n",
    "generator_folder = \"Generator\"\n",
    "graph_folder = \"Graph\"\n",
    "check_dir(generator_folder)\n",
    "check_dir(graph_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0fc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13802, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file,header = 0,usecols= [\"Messages\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b14cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13802it [00:01, 13222.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows()):\n",
    "    raw_data.append(json.loads(row[\"Messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.15 * len(raw_data))\n",
    "train_size = len(raw_data) - test_size * 2\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(raw_data, [train_size, test_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_to_arr(all_data, subset):\n",
    "    sub_arr = []\n",
    "    for i in subset.indices:\n",
    "        sub_arr.append(all_data[i])\n",
    "    return sub_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"train.json\"),\"w\") as f:\n",
    "    json.dump(subset_to_arr(raw_data, train_dataset),f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648cffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"valid.json\"),\"w\") as f:\n",
    "    json.dump(subset_to_arr(raw_data, valid_dataset),f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"test.json\"),\"w\") as f:\n",
    "    json.dump(subset_to_arr(raw_data, test_dataset),f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2knowledge = {}\n",
    "knowledge2path = {}\n",
    "source = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bdac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2know(path:list,knowledge:str):\n",
    "    not_exist_null = True\n",
    "    nums = len(path)\n",
    "    for p in path:\n",
    "        s = p[0].strip()\n",
    "        r = p[1].strip()\n",
    "        t = p[2].strip()\n",
    "        if s == '' or r == '' or t == '':\n",
    "            not_exist_null = False\n",
    "            continue\n",
    "        k = knowledge\n",
    "        if nums > 1:\n",
    "            patt=t\n",
    "            pattern = re.compile(patt)\n",
    "            result = pattern.search(knowledge)\n",
    "            k_span = result.span()\n",
    "            k_real = k_span[-1]\n",
    "            if len(knowledge)-1 > k_real:\n",
    "                k_real += 1\n",
    "            k = knowledge[:k_real]\n",
    "            knowledge = knowledge[k_real+1:]\n",
    "        str_path = s+\"\\t\"+r+\"\\t\"+t\n",
    "        item_p2k = {str_path:k}\n",
    "        item_k2p = {k:str_path}\n",
    "        path2knowledge.update(item_p2k)\n",
    "        knowledge2path.update(item_k2p)        \n",
    "    return not_exist_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4fdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_data(data):\n",
    "    datasets = []\n",
    "    NO_KNOWLEDGE_TOKEN = \"no_knowledge_used\"\n",
    "    for index, items in enumerate(data):\n",
    "        episode = {}\n",
    "        all_paths = []\n",
    "        parallel_paths = []\n",
    "        user_paths = []\n",
    "        assistant_paths = []\n",
    "        dialogs = []\n",
    "        for i,v in enumerate(items):\n",
    "            lens = len(items)\n",
    "            dialog = {}\n",
    "            knowledge = None\n",
    "            # v.__contains__(\"message\") and v[\"type\"] == \"chat\" and\n",
    "    #         if i==0 and v[\"sender\"] == \"assistant\":\n",
    "    #             #print(\"lalala...\")\n",
    "    #             continue\n",
    "            if v.__contains__(\"message\") and i < lens-1:\n",
    "                if items[i+1].__contains__(\"metadata\") and i < lens - 2:\n",
    "                    try:\n",
    "                        metadata = items[i+1][\"metadata\"]\n",
    "        #                 if items[i+1][\"action_id\"] == \"meta_thread/send_meta_message\" and items[i+2].__contains__(\"message\"):\n",
    "        #                     dialog[\"text\"] = v[\"message\"]\n",
    "        #                     dialog[\"knowledge\"] = NO_KNOWLEDGE_TOKEN    \n",
    "        #                     dialog[\"label\"] = items[i+2][\"message\"]\n",
    "        #                     dialogs.append(dialog)\n",
    "        #                     continue\n",
    "        #                 try:\n",
    "        #                     if items[i+2].__contains__(\"metadata\") and items[i+3].__contains__(\"metadata\"):\n",
    "        #                         dialog[\"text\"] = items[i+2][\"metadata\"][\"text\"]\n",
    "        #                         dialog[\"knowledge\"] = NO_KNOWLEDGE_TOKEN\n",
    "        #                         dialog[\"label\"] = items[i+3][\"metadata\"][\"text\"]\n",
    "        #                         dialogs.append(dialog)\n",
    "        #                         i+=2\n",
    "        #                 except:\n",
    "        #                     i+=2\n",
    "        #                     print(\"ggggggggg\")\n",
    "                        # 如果存在知识\n",
    "                        if metadata.__contains__(\"path\"):\n",
    "                            path_arr = metadata[\"path\"][1]\n",
    "                            # 不存在空知识\n",
    "                            if path2know(path_arr,metadata[\"path\"][-1]):\n",
    "                                all_paths.extend(path_arr)\n",
    "                                parallel_paths.append(path_arr)\n",
    "                                if v[\"sender\"] == \"user\":\n",
    "                                    assistant_paths.extend(path_arr)\n",
    "                                else:\n",
    "                                    user_paths.extend(path_arr)\n",
    "                                knowledge = metadata[\"path\"][-1]\n",
    "                                dialog[\"current_entity\"] = path_arr[0][0].strip()\n",
    "    #                             dialog[\"chose_relation\"] = path_arr[0][1].strip()\n",
    "                                dialog[\"target_entity\"] = path_arr[-1][2].strip()\n",
    "                                dialog[\"path\"] = path_arr\n",
    "                            # 存在空知识\n",
    "                            else:\n",
    "                                knowledge = NO_KNOWLEDGE_TOKEN\n",
    "                            dialog[\"text\"] = v[\"message\"]\n",
    "                            dialog[\"knowledge\"] = knowledge\n",
    "                            dialog[\"label\"] = items[i+2][\"message\"]\n",
    "                            dialog[\"sender\"] = v[\"sender\"]\n",
    "                            if dialog[\"text\"] != \"\" and dialog[\"label\"] != \"\":\n",
    "                                dialogs.append(dialog)\n",
    "                        # 不存在知识\n",
    "                        else:\n",
    "                            dialog[\"text\"] = v[\"message\"]\n",
    "                            dialog[\"knowledge\"] = NO_KNOWLEDGE_TOKEN\n",
    "                            dialog[\"label\"] = items[i+2][\"message\"]\n",
    "                            dialog[\"sender\"] = v[\"sender\"]\n",
    "                            if dialog[\"text\"] != \"\" and dialog[\"label\"] != \"\":\n",
    "                                dialogs.append(dialog)\n",
    "                            # print(\"hhhhhhhhhhhhh.....\")\n",
    "                    except:\n",
    "                        pass\n",
    "                        # print(\"ggggggggggg\")\n",
    "                else:\n",
    "                    try:\n",
    "                        dialog[\"label\"] = items[i+1][\"message\"]\n",
    "                        dialog[\"text\"] = v[\"message\"]\n",
    "                        dialog[\"knowledge\"] = NO_KNOWLEDGE_TOKEN\n",
    "                        dialog[\"sender\"] = v[\"sender\"]\n",
    "                        if dialog[\"text\"] != \"\" and dialog[\"label\"] != \"\":\n",
    "                            dialogs.append(dialog)\n",
    "                    except:\n",
    "                        #print(items[i])\n",
    "                        #print(\"oooooooooooooooooooo\")\n",
    "                        pass\n",
    "\n",
    "        if len(dialogs) > 1:\n",
    "            episode[\"dialog\"] = dialogs\n",
    "            episode[\"all_paths\"] = all_paths\n",
    "            episode[\"parallel_paths\"] = parallel_paths\n",
    "            episode[\"user_paths\"] = user_paths\n",
    "            episode[\"assistant_paths\"] = assistant_paths\n",
    "            datasets.append(episode)\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus = make_generator_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = make_generator_data(train_dataset)\n",
    "valid_corpus = make_generator_data(valid_dataset)\n",
    "test_corpus = make_generator_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776994db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "entities = []\n",
    "triples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fe44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kg(corpus):\n",
    "    for j in tqdm(corpus):\n",
    "        for i in j[\"all_paths\"]:\n",
    "            i_0 = i[0].strip()\n",
    "            i_1 = i[1].strip()\n",
    "            i_2 = i[2].strip()\n",
    "#             assert i_0 in all_entities\n",
    "#             assert i_2 in all_entities\n",
    "#             assert i_1 in all_relations\n",
    "            if i_0 not in entities:\n",
    "                entities.append(i_0)\n",
    "            if i_2 not in entities:\n",
    "                entities.append(i_2)\n",
    "            if i_1 not in relations:\n",
    "                relations.append(i_1)\n",
    "            triple = f\"{i_0}\\t{i_1}\\t{i_2}\"\n",
    "            if triple not in triples:\n",
    "                triples.append(triple)\n",
    "#             assert triple in all_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 8752/8752 [00:02<00:00, 3786.48it/s]\n",
      "100%|█████████████████████████████████████| 1881/1881 [00:00<00:00, 2466.67it/s]\n",
      "100%|█████████████████████████████████████| 1871/1871 [00:00<00:00, 2224.61it/s]\n"
     ]
    }
   ],
   "source": [
    "make_kg(train_corpus)\n",
    "make_kg(valid_corpus)\n",
    "make_kg(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122899b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9756\n",
      "329\n",
      "20576\n"
     ]
    }
   ],
   "source": [
    "print(len(entities))\n",
    "print(len(relations))\n",
    "print(len(triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(generator_folder,\"all.json\"),\"w\") as f:\n",
    "    json.dump(all_corpus, f, indent=4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(generator_folder,\"train.json\"),\"w\") as f:\n",
    "    json.dump(train_corpus,f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(generator_folder,\"valid.json\"),\"w\") as f:\n",
    "    json.dump(valid_corpus,f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(generator_folder,\"test.json\"),\"w\") as f:\n",
    "    json.dump(test_corpus,f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(graph_folder,\"entities.txt\"),\"w\") as f:\n",
    "    for i in entities:\n",
    "        f.write(i.strip())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f346ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(graph_folder,\"relations.txt\"),\"w\") as f:\n",
    "    for i in relations:\n",
    "        f.write(i.strip())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc790a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(graph_folder,\"sub_triples.txt\"),\"w\") as f:\n",
    "    for i in triples:\n",
    "        f.write(i.strip())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(graph_folder,\"path2knowledge.json\"),\"w\") as f:\n",
    "    json.dump(path2knowledge,f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(graph_folder,\"knowledge2path.json\"),\"w\") as f:\n",
    "    json.dump(knowledge2path,f,indent = 4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f54326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
